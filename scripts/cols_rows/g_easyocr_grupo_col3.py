import json
import os
import re
from collections import defaultdict
from pathlib import Path

import easyocr
import pandas as pd
from tqdm import tqdm

# Directorios de entrada y salida
TIPO = "grupo_parlamentario"
COLUMN_NAME = "columna_3"
DIR_NAMES_CSV = "/home/nahumfg/GithubProjects/parliament-voting-records/data/col_rows/dir_names.csv"
PARQUET = "/home/nahumfg/GithubProjects/parliament-voting-records/data/col_rows/grupo_parlamentario_images.parquet"
OUTPUT_DIR = (
    f"/home/nahumfg/GithubProjects/parliament-voting-records/data/col_rows/{TIPO}_{COLUMN_NAME}"
)

# Configuraci√≥n de EasyOCR
USE_GPU = True  # Cambiar a False si no tienes GPU con CUDA
LANGUAGES = ["es"]  # Espa√±ol
BATCH_SIZE = 1024  # N√∫mero de im√°genes a procesar en lote (ajustar seg√∫n VRAM disponible)

# Variable global para el reader de EasyOCR (se inicializa una sola vez)
reader = None


def natural_sort_key(text):
    """
    Clave para ordenamiento natural (fil_1.png, fil_2.png, ..., fil_10.png)
    """

    def atoi(text):
        return int(text) if text.isdigit() else text

    return [atoi(c) for c in re.split(r"(\d+)", text)]


def extract_document_id_and_page(dir_name):
    """
    Extrae el UUID del documento y el n√∫mero de p√°gina del dir_name.
    Ejemplo: '000058a7-4618-53af-82f8-13266eef3ace_page003_' ->
             ('000058a7-4618-53af-82f8-13266eef3ace', 'page003')
    """
    # Patr√≥n: UUID_pageXXX_
    match = re.match(r"^([a-f0-9\-]+)_(page\d+)_$", dir_name)
    if match:
        return match.group(1), match.group(2)
    return None, None


def initialize_reader():
    """
    Inicializa el reader de EasyOCR una sola vez.
    Esto es importante porque la inicializaci√≥n carga los modelos en GPU/memoria.
    """
    global reader
    if reader is None:
        print(f"\nüîß Inicializando EasyOCR...")
        print(f"   - GPU habilitada: {USE_GPU}")
        print(f"   - Idiomas: {LANGUAGES}")
        reader = easyocr.Reader(LANGUAGES, gpu=USE_GPU, verbose=False)
        print("‚úì EasyOCR inicializado correctamente")
    return reader


def apply_ocr_to_image(image_path, ocr_reader):
    """
    Aplica EasyOCR a una imagen y retorna el texto extra√≠do.
    Retorna string vac√≠o si la imagen no existe o hay error.

    Args:
        image_path: Ruta a la imagen
        ocr_reader: Instancia de easyocr.Reader
    """
    try:
        if not os.path.exists(image_path):
            return ""

        # Aplicar OCR con EasyOCR
        # readtext retorna una lista de tuplas: (bbox, text, confidence)
        result = ocr_reader.readtext(image_path, detail=0)  # detail=0 retorna solo texto

        # Unir todos los textos detectados con saltos de l√≠nea
        text = "\n".join(result)
        return text.strip()
    except Exception as e:
        return ""


def process_images_in_batches(image_paths, ocr_reader, batch_size):
    """
    Procesa m√∫ltiples im√°genes en lotes para mayor eficiencia.

    Args:
        image_paths: Lista de rutas de im√°genes
        ocr_reader: Instancia de easyocr.Reader
        batch_size: Tama√±o del lote

    Returns:
        Lista de textos extra√≠dos en el mismo orden que image_paths
    """
    results = []

    for i in range(0, len(image_paths), batch_size):
        batch = image_paths[i : i + batch_size]

        for img_path in batch:
            text = apply_ocr_to_image(img_path, ocr_reader)
            results.append(text)

    return results


def process_images():
    """
    Procesa las im√°genes:
    1. Lee dir_names.csv
    2. Filtra congresistas_images.parquet
    3. Aplica OCR a cada imagen usando EasyOCR con GPU
    4. Agrupa resultados por documento y p√°gina
    5. Guarda JSONs
    """
    print("=" * 70)
    print("üöÄ INICIANDO PROCESAMIENTO DE OCR EN COLUMNAS (EasyOCR + GPU)")
    print("=" * 70)

    # Inicializar EasyOCR
    ocr_reader = initialize_reader()

    # 1. Leer dir_names.csv
    print("\nüìÇ 1. Leyendo dir_names.csv...")
    if not os.path.exists(DIR_NAMES_CSV):
        print(f"‚ùå ERROR: No se encuentra {DIR_NAMES_CSV}")
        return

    dir_names_df = pd.read_csv(DIR_NAMES_CSV)
    print(f"‚úì Se encontraron {len(dir_names_df)} dir_names totales")

    # Filtrar dir_names que ya fueron procesados
    # Los JSONs se guardan con el UUID (sin _pageXXX_), as√≠ que extraemos el UUID de cada dir_name
    existing_jsons = set()
    if os.path.exists(OUTPUT_DIR):
        existing_jsons = {
            f.replace(".json", "") for f in os.listdir(OUTPUT_DIR) if f.endswith(".json")
        }
        print(f"‚úì Se encontraron {len(existing_jsons)} documentos ya procesados")

    # Filtrar dir_names cuyos UUIDs ya existen
    def get_uuid_from_dirname(dir_name):
        """Extrae el UUID del dir_name (quita _pageXXX_)"""
        doc_id, _ = extract_document_id_and_page(dir_name)
        return doc_id

    dir_names_df["uuid"] = dir_names_df["dir_name"].apply(get_uuid_from_dirname)
    dir_names_df_filtered = dir_names_df[~dir_names_df["uuid"].isin(existing_jsons)]
    dir_names_set = set(dir_names_df_filtered["dir_name"].tolist())

    print(f"‚úì Despu√©s de filtrar procesados: {len(dir_names_set)} dir_names a procesar")

    if len(dir_names_set) == 0:
        print("‚ö†Ô∏è  No hay dir_names nuevos para procesar (todos ya fueron procesados)")
        return

    # 2. Leer y filtrar parquet
    print("\nüìÇ 2. Leyendo y filtrando congresistas_images.parquet...")
    if not os.path.exists(PARQUET):
        print(f"‚ùå ERROR: No se encuentra {PARQUET}")
        return

    df = pd.read_parquet(PARQUET)
    print(f"‚úì Archivo parquet cargado: {len(df)} registros totales")

    # Filtrar por dir_names y column=COLUMN_NAME
    df_filtered = df[df["dir_name"].isin(dir_names_set) & (df["column"] == COLUMN_NAME)]
    print(f"‚úì Despu√©s de filtrar por dir_names y {COLUMN_NAME}: {len(df_filtered)} registros")

    if len(df_filtered) == 0:
        print("‚ö†Ô∏è  No hay registros para procesar despu√©s de filtrar")
        return

    # 3. Crear directorio de salida
    print(f"\nüìÅ 3. Creando directorio de salida...")
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    print(f"‚úì Directorio de salida: {OUTPUT_DIR}")

    # 4. Agrupar por documento
    print("\nüîç 4. Agrupando datos por documento...")
    documents = defaultdict(lambda: defaultdict(list))

    # Agrupar las filas por documento y p√°gina
    for _, row in df_filtered.iterrows():
        dir_name = row["dir_name"]
        image_path = row["image_path"]
        image_name = row["image_name"]

        doc_id, page = extract_document_id_and_page(dir_name)
        if doc_id and page:
            documents[doc_id][page].append((image_path, image_name))

    print(f"‚úì Se encontraron {len(documents)} documentos √∫nicos")
    print(f"‚úì Total de p√°ginas a procesar: {sum(len(pages) for pages in documents.values())}")

    # 5. Procesar cada documento
    print("\nüî¨ 5. Procesando OCR en im√°genes con EasyOCR...")
    print(f"‚öôÔ∏è  Procesamiento en GPU (batch_size={BATCH_SIZE})")
    total_images = len(df_filtered)

    with tqdm(total=total_images, desc="Procesando im√°genes", unit="img") as pbar:
        for doc_id, pages in documents.items():
            doc_result = {}

            # Procesar cada p√°gina del documento
            for page, images in sorted(pages.items()):
                # Ordenar im√°genes por nombre usando ordenamiento natural
                images_sorted = sorted(images, key=lambda x: natural_sort_key(x[1]))

                # Extraer solo las rutas de im√°genes
                image_paths = [img_path for img_path, _ in images_sorted]

                # Procesar im√°genes en lotes
                page_texts = []
                for img_path in image_paths:
                    text = apply_ocr_to_image(img_path, ocr_reader)
                    page_texts.append(text)
                    pbar.update(1)

                doc_result[page] = page_texts

            # Guardar JSON del documento
            json_filename = f"{doc_id}.json"
            json_path = os.path.join(OUTPUT_DIR, json_filename)

            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(doc_result, f, ensure_ascii=False, indent=2)

    print(f"\n‚úÖ ¬°Proceso completado!")
    print(f"üìä Resumen:")
    print(f"   - Documentos procesados: {len(documents)}")
    print(f"   - Im√°genes procesadas: {total_images}")
    print(f"   - JSONs guardados en: {OUTPUT_DIR}")
    print("=" * 70)


def main():
    process_images()


if __name__ == "__main__":
    main()
